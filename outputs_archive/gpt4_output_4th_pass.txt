Supervised learning (SL) includes classification and regression, assigning labels and values respectively. ML uses instances as input vectors and aims to learn target concept mappings from input-output pairs. The hypothesis class is limited by data and inductive learning derives functions from training sets, with testing sets ensuring model generalization. Decision trees, with decision nodes and edges, use attributes to predict outcomes, and are optimized for accuracy and overfitting prevention. They handle Boolean functions, but complex ones like XOR increase computational load. ALGs like ID3 improve decision tree efficiency by selecting attributes that maximize information gain and minimize entropy. Continuous outcomes in trees involve specific splitting criteria and fitting ALGs. ML models use squared error minimization and matrix multiplication for polynomial regression. Handling data noise and using cross-validation are crucial in ML for estimating model accuracy and optimal complexity. Skepticism around using RGB for categorical data like hair color persists. NN concepts include mean squared error importance and their similarity to brain operations. NNs use artificial neurons and adjust inputs and weights through algos like the Perceptron Rule and gradient descent, the latter for non-linear cases. Separate adjustments are made for linear separability, gradient direction, and local optima avoidance. Sigmoid functions enable smooth transition states for backpropagation in NNs. NNs have evolved to include layers for complex, discontinuous function modeling, with cross-validation helping select the optimal architecture. Emphasis on architecture, weight initialization, and simplicity, according to Occam's razor, is critical. The shift towards instance-based learning (IBL) is noted, where
ML challenges & opt. strategies are discussed. k-NN alg's noise sensitivity, overfitting, & high space-time req. during learning/query phases noted. Compares to linear regression w/ higher learning cost but faster query time. Emphasizes selecting suitable distance metrics & feature significance for k-NN, notes Curse of Dimensionality's impact & suggests weighted distance func. to counter. Optimal 'k' selection & weighted avgs mentioned for adaptability in determining influence, alongside locally weighted regression for better prediction accuracy by fitting models to local data. Decision trees, NNs, & linear regression can be improved with complex funcs. over avgs. Computational learning theory's learner-teacher dynamics, true error, epsilon exhaustion, & sample complexity bounds discussed, linking hypothesis space size to error & failure rates. Agnostic learning & infinite hypothesis space challenges acknowledged, w/ future discussions on the latter. Ensemble learning, esp. boosting, is highlighted for its effectiveness in integrating simple rules & aggregating diverse evidence. It leverages randomness in data subsets, reduces expected error & increases prediction precision in housing data. Bagging & boosting are methods that improve ensemble outcomes by averaging data subset predictions & focusing on difficult data points, respectively. Boosting also increases focus on difficult examples by using a weighted mean for error calculation. ML error rate is explained as the mismatch between actual & predicted outcomes due to example distribution during training/testing. Boosting is connected to learning from errors & distribution importance, defining a weak learner as an alg w/ an error rate under 50%. Boosting aims to create a strong classifier from weak ones by adjusting learners to outperform chance. (Note: Abbreviations were used as per instruction. However, concepts/terms like 'weighted average' and 'decision trees' do currently not have widely recognized abbreviations and were thus kept unabbreviated.)
The alg adjusts ex weights iteratively based on perf, beginning with unif dist. Alpha values are calc'd from errs, influencing which weak learners are selected. Final hyp is a wt'd avg of these learners. Boosting targets hard ex, reduces misclass and errs, similar to neural nets and wt'd nearest neighbor alg. SVMs find optimal hyperplanes via quad prog, focusing on support vectors, using dot products and high-dim proj w/ kernel tricks for data separation and err balance. Boosting and SVMs reduce overfitting and improve ML perf by combining weak hyps. Boosting may cause overfitting, esp. when strong learners are used or due to pink noise. Computational learning theory emphasizes the need for proper problem def, alg analysis, and qty of training data. In ML, interactive learning between learners and teachers, as illustrated by the "20 questions" game, aims to efficiently reduce hyp space. Challenges include large potential hyp numbers and exponential time for guessing. Sample complexity is prioritized over computation, despite the limitations of linear "20 questions" method. A mistake-bound model is suggested for minimizing errs. Self-sufficient ML relies on errors without teacher guidance for refining alg and removing irrelevant variables. Optimal learning requires ex to differ by only one variable.
ML highlights the significance of hypothesis space's size and complexity, focusing on PAC learning w/ error ε and confidence δ to ensure high-confidence predictions. Computational and sample complexity are crucial, and version space must include the true concept, emphasizing the uniform selection in scarce data scenarios. Haussler's Theorem relates ε, hypothesis space size, and sample size for error reduction. ML compares to CS complexities, valuing data acquisition and learnable strategies. Hypothesis complexity is tested in ML through sample complexity, error tolerance, and failure rates, with agnostic learning choosing the best hypothesis sans target space assumptions. High VC dimensions indicate large learning capacity but require more data, with learnability feasible if VC dimensions are finite. Bayesian learning applies Bayes' Rule for probable hypothesis identification, essential in AI, integrating priors via MAP hypothesis or ignoring them in maximum likelihood approaches.
V:S ratio in ML lwrs w/ noise & incomp conc knowl. Max likelihood est nmlz data w/ norm noise. Simplif techniques in Bayesian lrng & grad desc incl logs for sum prod & const discard. CS7641 showcases Bayesian eval ind of hyp class, while linear reg > consts & data means. Logarithms preserve max sols & inform theory aids optimal code. ML pref simple models, dec trees w/ fewer nodes to avd overfitting; yet, Bayesian minimizes error consistent w/ Occam's Razor. Bayesian lrng sup on avg w/ comprehensive hyp assess & Bayes Nets recommended for complex distribs. Bayes Nets, using DAGs & CPTs, simplify vars for inference. Sampling is crucial for approx inference in ML; marginalization & Bayes' rule pred in uncertain conditions. Naive Bayes for spam detec efficient but risks overfitting, curbed by smoothing; faces inf datas & cond indep issues. Bayesian inference & nets tackle NP-complete probs in ML, useful for classification w/ missing attrs. Sup lrng transitions to UL; discusses ML's randomized optim for ind like chem eng. (Note: Some terms like "Bayesian Learning" were not abbreviated as their abbreviations were not provided in the original text.)
ML fine-tunes neural net and decision tree params to reduce error. A quiz, then an optimization prob using calculus, specifically iterative gradient descent via Newton's method, is introduced. For complex funcs, randomized opt or Hill Climbing alg might be used to evade local optima. "Guess My Word" opt employs a fitness func, alg w/ neighbor func, and Random Restart Hill Climbing, achieving global optima in avg 5.39 steps. V is set at 29.78, with enumeration suggested for small input ranges. To circumvent local optima, algs like quick random restarts and keeping track of visited points are used, with randomized opt often being more efficient than full evals. Simulated Annealing, similar to Metropolis-Hastings, uses metaphorical temp changes, adjusts exploration and exploitation balance, and prefers global optima as temp drops, with too fast cooling risking suboptimal sols. The text discusses GAs, inspired by natural evolution, using Boltzmann distribution for selection, fitness-based strategies, crossover, and mutation for opt. GAs assume solution segment independence for effective crossover and compare one-point and uniform crossover methods. It considers limitations of minimal memory algs like hill climbing and simulated annealing, but notes the latter can represent problem space structure. It advocates mixing alg concepts and prob distribution modeling, as in Mimic, and highlights using prob distributions in evolutionary algs. Dependability trees and minimizing KL divergence are used for feature interdependencies in ML, with Prim's alg used to maximize mutual information in dependency trees. MIMIC is found efficient for prob sampling through dependency trees and stresses precise prob distribution identification for solving Boolean opt probs, offering a balance between complexity and capturing relationships to find optimal prob distributions, especially as fitness values near uniformity. The text addresses clustering and optimization algs in ML, noting the challenges of estimating prob distributions from uniform samples and advising generating more samples for better theta approximation (>2). For complex tasks, Mimic uses data structure for efficient solutions, despite slower iteration times and overfitting risks, outperforming methods like Simulated Annealing. In supervised learning, algs generalize from labeled data, while UL, like clustering, finds dense representations from unlabeled data, using various measures for clusters without metric spaces. Single Linkage Clustering is mentioned.
SLC clusters 'n' pts into 'K' cls w/ cubic time in 'n', which may become linear w/ data structures like Fibonacci heaps. CS7641 covers SLC for 'K=2' & K-means, latter improves elongated cls handling by iteratively recalculating centers. K-means minimizes distance-based scores until convergence depends on initial center placement & uses tie-breaking. EM does soft cls, alternating between likelihood assignments & center updates based on probabilities, likened to K-means w/ binary probabilities. Cls alg properties of richness, scale-invariance, & consistency are domain-specific, can't be simultaneously fulfilled (Kleinberg). 'Wrap' & 'filter' are fsel techniques w/ trade-offs. Transformation prioritized over fsel for preds & dimensionality mgmt., w/ PCA & ICA for data transformation. PCA uses eigenvalues to reduce dims & maintain variance orthogonally. ICA separates ind. variables w/o orthogonality, useful in the Cocktail Party Problem. RCA noted for speed but with info loss. LDA req. labels for supervised dimensionality reduction. Grad projects prove ICA's effectiveness in data structure revelation. Lecture discusses ICA vs. PCA in UL, w/ ICA more favorable in complex situations. UL, DP, & RL emphasized for homework & projects. Shannon's info theory foundational to ML, introducing entropy & variable-length encoding (e.g., Morse code), mutual information (I), and KL divergence in SL. MDPs in RL discuss maximizing expected long-term rewards via policy optimization.
RL, rooted in psych. and CS, focuses on maximizing rewards. Finite/infinite horizons affect policies, with finite prompting changes due to time limits. Regret and gamma address the time value of rewards, setting Rmax/(1-gamma) reward bounds. Optimal policy (π*) seeks to maximize discounted rewards, distinct from non-deterministic policies focused on avg rewards. Bellman Equation, crucial for solving MDPs, finds π* through iterative alg. utilizing value iteration and action-transition analysis. Policy iteration in RL refines policies using linear methods for expected utility max. MDPs leverage discounting for finite valuing of infinite rewards, overcoming the immortality problem. RL, seen as an API in MDPs, learns to maximize rewards based on transitions. Model-based RL uses planners, models, simulations, and algs. like value/policy iteration. Value funcs and Bellman equations are key for π* but computationally demanding. Q-learning bypasses MDP solutions by updating Q estimates (Q̂) iteratively without known rewards or transitions. Strategies like αt = 1/t ensure convergence and all state-action pairs need infinite visits for accurate sampling. Epsilon Greedy Exploration and methods like random restarts, simulated annealing address exploration-exploitation trade-off and local minima. Finally, game theory extends RL to multi-agent, zero-sum games with perfect info, integrating model learning, planning, and exploration-exploitation strategies. RL ignores function approximation and broader ML issues in this context.
Player maximizes or minimizes rewards utilizing MDPs and decision trees. RL likened to game theory strategies. Minimax strategy key for game trees and AI alg's, esp. in alpha-beta pruning. Challenges in translating game trees to matrices noted, with matrices aiding in game value determination. Minipoker highlights hidden info and complex games, where mixed strategies outperform pure strategies due to unpredictability and stable expected values. Calculating probabilities essential in optimal strategies within a "bow tie" payoff space. Von Neumann's theorem noted for non-deterministic games; minimax/maximin strategies not always aligning in non-zero-sum games. Game value constant across different strategies, emphasizing rationality. In zero-sum games like Prisoner's Dilemma, outcomes of defection or cooperation impact jail time. Nash equilibrium is a state where no player benefits from a strategy change, given others' strategies, occurring in all finite games. Repeated gameplay, threats, sunk costs, trust, and game theory principles impact decision-making. Game theory explores strategies and incentives, with continued probability (gamma) influencing coop. or defection tendencies. Nash equilibria emerge from mutual coop. or defection with no deviation incentive. Repeated games can foster coop. through potential retaliation, with any payoff better than a minmax profile sustainable indefinitely. The Prisoner's Dilemma examined, with coop. leading to -1 avg. payoff each within possible outcomes. Minmax profiles analyzed, showing mixed strategies as superior to pure in securing scores against adversaries. Feasible preferable acceptable region identified, with Nash equilibrium maintained through adequate discount factors and coordinated strategies, deviations met with punishment. Strategies like grim trigger and tit for tat differ in retaliation, promoting ongoing coop. Nash equilibrium also critical in stochastic games, computed using linear programming, suggesting that sub-game perfect Nash equilibria with adaptive Pavlov-like strategies are computable.
Poly time for 2-player games is achieved by analyzing Nash Equilibrium in stochastic games, generalizing MDPs and reducible to models like zero-sum. Discount factors affect outcomes and strat in these games vs. repeated games. In ML, zero-sum game Q values determine outcomes for 2-player competitive scenarios; 3-player games are considered general-sum. Mini-max Q learning update converges to a unique Q* but solving zero-sum stochastic games is hard. For general-sum games, Nash Equilibrium faces non-convergence and NP-hard computational challenges. Coop games use "coco values" for collaborative strategies and connect to iterated prisoner's dilemma and RL. Topics include credible threats, Folk Theorem's equilibria, ED in equilibrium computation, and cognitive hierarchy theories. The import of ongoing research and coop is highlighted.